{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc47ea2",
   "metadata": {},
   "source": [
    "# Tp Ciencia de datos 2025\n",
    "\n",
    "## Grupo 2\n",
    "**Integrantes:**\n",
    "- Francisco Lucich\n",
    "- Esteban Luna\n",
    "- Francisco Moretti\n",
    "- Tomás Zubik\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2d31f",
   "metadata": {},
   "source": [
    "# Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17533d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos 'all' para obtener tanto el set de entrenamiento como el de test\n",
    "# Removemos headers, footers y quotes para simplificar el texto\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una lista donde cada elemento es el texto de un documento\n",
    "newsgroups_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f64509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un array de NumPy con números enteros que representan la categoría de cada documento.\n",
    "newsgroups_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una lista con los nombres correspondientes a cada número en .target.\n",
    "newsgroups_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e7937",
   "metadata": {},
   "source": [
    "# Fase 1: Carga, Exploración Inicial y Limpieza Básica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a04865",
   "metadata": {},
   "source": [
    "**1: carga del dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77845dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({'texto': newsgroups_data.data, 'target': newsgroups_data.target})\n",
    "# Mapear los números de target a los nombres de las categorías\n",
    "df['categoria'] = df['target'].apply(lambda i: newsgroups_data.target_names[i])\n",
    "# Ahora puedes descartar la columna 'target' si lo deseas\n",
    "# df = df.drop('target', axis=1)\n",
    "print(df.head())\n",
    "print(f\"Nombres de las categorías: {newsgroups_data.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d564a4",
   "metadata": {},
   "source": [
    "**2: Exploracion inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9dc1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e52079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ed735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.inspect() no existe result: AttributeError: 'DataFrame' object has no attribute 'inspect''ArithmeticError\n",
    "\n",
    "# describe() muestra estadísticas descriptivas de las columnas numéricas del DataFrame,\n",
    "# como media, desviación estándar, mínimos, máximos y percentiles.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f1b20",
   "metadata": {},
   "source": [
    "**2.1 analisis de categorias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La función value_counts() cuenta la cantidad de ocurrencias de cada valor en una columna.\n",
    "#sort_index() ordena los resultados por el índice (en este caso, las categorías).\n",
    "df.value_counts('categoria').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade9136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización: Gráfico de barras de la distribución de documentos por categoría\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Contar documentos por categoría\n",
    "conteo_categorias = df['categoria'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=conteo_categorias.index, y=conteo_categorias.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.yticks(range(0, conteo_categorias.max() + 100, 100))\n",
    "plt.xlabel('Categoría')\n",
    "plt.ylabel('Cantidad de documentos')\n",
    "plt.title('Distribución de documentos por categoría')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27ae5a",
   "metadata": {},
   "source": [
    "**3. limpieza basica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "df['texto_limpio'] = df['texto'].str.lower()  # Convertir a minúsculas\n",
    "\n",
    "df['texto_limpio'] = df['texto_limpio'].str.replace(r'\\d+', '', regex=True)  # Eliminar números\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)  # Crear un traductor para eliminar puntuación\n",
    "df['texto_limpio'] = df['texto_limpio'].apply(lambda x: x.translate(translator))  # Eliminar puntuación\n",
    "\n",
    "df['texto_limpio'] = df['texto_limpio'].str.replace(r'\\s+', ' ', regex=True)  # Eliminar espacios en blanco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texto_limpio'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b8333",
   "metadata": {},
   "source": [
    "# Fase 2: Preprocesamiento con NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5673d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d1704",
   "metadata": {},
   "source": [
    "**1. Tokenizacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens']= df['texto_limpio'].apply(nltk.word_tokenize)  # Tokenización\n",
    "\n",
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a12c2",
   "metadata": {},
   "source": [
    "**2. Eliminacion de Stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8367a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_sin_stopwords'] = df['tokens'].apply(lambda x: [word for word in x if word not in nltk.corpus.stopwords.words('english')])\n",
    "df['tokens_sin_stopwords'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords_personalizadas = {'would', 'could', 'also', 'one', 'get', 'like', 'use', 'subject', 'writes', 'article'}\n",
    "\n",
    "# Unir ambas listas\n",
    "stopwords_actualizadas = stopwords.union(stopwords_personalizadas)\n",
    "\n",
    "# Aplicar la eliminación de stopwords (incluyendo las personalizadas)\n",
    "df['tokens_sin_stopwords'] = df['tokens'].apply(lambda x: [word for word in x if word not in stopwords_actualizadas])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ed496",
   "metadata": {},
   "source": [
    "**2. Lematización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Inicializar el lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Aplicar lematización\n",
    "df['tokens_lematizados'] = df['tokens_sin_stopwords'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "df[['tokens_sin_stopwords', 'tokens_lematizados']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb7145",
   "metadata": {},
   "source": [
    "*Comparación Lematización y Stemming*\n",
    "\n",
    "La lematización reduce las palabras a su forma base (lema) utilizando un enfoque basado en el significado y la gramática. Es más preciso pero más lento. Mientras que el stemming recorta las palabras a su raíz básica sin considerar el contexto gramatical. Es más rápido pero menos preciso.\n",
    "\n",
    "Usaríamos lemmatización cuando se necesita precisión y el significado de las palabras es importante (por ejemplo, análisis semántico), y stemming cuando se valora más la  velocidad y no importa perder algo de precisión (por ejemplo, motores de búsqueda)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8d87c",
   "metadata": {},
   "source": [
    "**4. Análisis de Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Combinar todos los tokens lematizados en una sola lista\n",
    "\n",
    "todos_los_tokens = [token for tokens in df['tokens_lematizados'] for token in tokens]\n",
    "\n",
    "# Calcular la frecuencia de cada token\n",
    "frecuencia_tokens = FreqDist(todos_los_tokens)\n",
    "\n",
    "tokens_mas_frecuentes = frecuencia_tokens.most_common(30)\n",
    "print(\"Tokens más frecuentes:\", tokens_mas_frecuentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f734ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparar datos para el gráfico\n",
    "tokens, frecuencias = zip(*tokens_mas_frecuentes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(tokens), y=list(frecuencias), palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Tokens más frecuentes después del preprocesamiento')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f958d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una función para calcular frecuencias por categoría\n",
    "def frecuencia_por_categoria(categoria):\n",
    "    tokens_categoria = [token for tokens in df[df['categoria'] == categoria]['tokens_lematizados'] for token in tokens]\n",
    "    return FreqDist(tokens_categoria)\n",
    "\n",
    "# Seleccionar algunas categorías para análisis\n",
    "categorias_seleccionadas = df['categoria'].unique()[:5] \n",
    "\n",
    "for categoria in categorias_seleccionadas:\n",
    "    frecuencia_categoria = frecuencia_por_categoria(categoria)\n",
    "    tokens_categoria, frecuencias_categoria = zip(*frecuencia_categoria.most_common(10))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(tokens_categoria), y=list(frecuencias_categoria), palette=\"viridis\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title(f'Tokens más frecuentes en la categoría: {categoria}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f3d3c",
   "metadata": {},
   "source": [
    "# Fase 3: Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b13fb4",
   "metadata": {},
   "source": [
    "**1. Preparación final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b85b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar los tokens lematizados de cada documento en una sola cadena de texto\n",
    "df['texto_final_procesado'] = df['tokens_lematizados'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "\n",
    "print(df['texto_final_procesado'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0be627",
   "metadata": {},
   "source": [
    "**2. Vectorización con CountVectorizer (Bag-of-Words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e73319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Inicializar CountVectorizer \n",
    "vectorizador = CountVectorizer(max_features=1000, min_df=5, max_df=0.7)\n",
    "\n",
    "# Aplicar el vectorizador a la columna texto_final_procesado\n",
    "matriz_count = vectorizador.fit_transform(df['texto_final_procesado'])\n",
    "\n",
    "# Imprimir la forma de la matriz resultante\n",
    "print(f\"Forma de la matriz Bag-of-Words: {matriz_count.shape}\")\n",
    "\n",
    "# Mostrar una parte del vocabulario aprendido por el vectorizador\n",
    "vocabulario = vectorizador.get_feature_names_out()\n",
    "print(f\"Parte del vocabulario aprendido: {vocabulario[:40]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a1485",
   "metadata": {},
   "source": [
    "**3. Vectorización con TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inicializar TfidfVectorizer \n",
    "vectorizador_tfidf = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.7)\n",
    "\n",
    "# Aplicar el vectorizador TF-IDF a la columna texto_final_procesado\n",
    "matriz_tfidf = vectorizador_tfidf.fit_transform(df['texto_final_procesado'])\n",
    "\n",
    "# Imprimir la forma de la matriz resultante\n",
    "print(f\"Forma de la matriz TF-IDF: {matriz_tfidf.shape}\")\n",
    "\n",
    "# Mostrar una parte del vocabulario aprendido por el vectorizador\n",
    "vocabulario_tfidf = vectorizador_tfidf.get_feature_names_out()\n",
    "print(f\"Parte del vocabulario aprendido: {vocabulario_tfidf[:40]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d886fb",
   "metadata": {},
   "source": [
    "*Comparación entre CountVectorizer y TfidfVectorizer*\n",
    "\n",
    "El CountVectorizer genera una matriz donde cada valor representa la **frecuencia absoluta** de un token en un documento.\n",
    "Un valor alto indica que el token aparece muchas veces en ese documento, pero no considera si el token es común en otros documentos.\n",
    "Mientras que en el TfidfVectorizer cada valor representa la **importancia** del token en un documento, calculada como el producto de su frecuencia (TF) y su rareza inversa (IDF).\n",
    "Un valor alto indica que el token es frecuente en el documento pero raro en el resto del corpus, lo que lo hace más relevante.\n",
    "\n",
    "\n",
    "TF-IDF penaliza los términos que son comunes en muchos documentos (como \"the\" o \"and\"), dándoles menor peso.\n",
    "Esto mejora la representación del texto al destacar términos más específicos y relevantes para cada documento, lo que es útil para tareas como clasificación o clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082dc5b",
   "metadata": {},
   "source": [
    "**4. Visualización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a37f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combinar todo el texto de la columna texto_final_procesado\n",
    "texto_completo = ' '.join(df['texto_final_procesado'])\n",
    "\n",
    "# Crear la nube de palabras\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(texto_completo)\n",
    "\n",
    "# Mostrar la nube de palabras\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Ocultar los ejes\n",
    "plt.title('Nube de Palabras - Términos Más Prominentes', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
