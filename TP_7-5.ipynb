{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86186d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorem ipsum dolor sit amet consectetur adipiscing elit sed euismod orci ut commodo placerat sapien sapien convallis nulla nec luctus sapien turpis sit amet massa phasellus facilisis magna et ligula elementum sed volutpat sem malesuada donec sed tincidunt turpis pellentesque tincidunt ante nec justo congue in feugiat metus vehicula nulla facilisi vivamus sagittis lorem a augue viverra at dapibus ex ultrices vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae pellentesque tincidunt leo quis dui gravida vitae vulputate lorem feugiat\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "texto = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed euismod, orci ut commodo placerat, sapien sapien convallis nulla, nec luctus sapien turpis sit amet massa. Phasellus facilisis magna et ligula elementum, sed volutpat sem malesuada. Donec sed tincidunt turpis. Pellentesque tincidunt ante nec justo congue, in feugiat metus vehicula. Nulla facilisi. Vivamus sagittis lorem a augue viverra, at dapibus ex ultrices. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Pellentesque tincidunt leo quis dui gravida, vitae vulputate lorem feugiat.'\n",
    "\n",
    "\n",
    "texto_minusculas = texto.lower()\n",
    "\n",
    "texto_sin_signos = texto_minusculas.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(texto_sin_signos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e5ae7",
   "metadata": {},
   "source": [
    "tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae83bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') \n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccd9dcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur', 'adipiscing', 'elit', 'sed', 'euismod', 'orci', 'ut', 'commodo', 'placerat', 'sapien', 'sapien', 'convallis', 'nulla', 'nec', 'luctus', 'sapien', 'turpis', 'sit', 'amet', 'massa', 'phasellus', 'facilisis', 'magna', 'et', 'ligula', 'elementum', 'sed', 'volutpat', 'sem', 'malesuada', 'donec', 'sed', 'tincidunt', 'turpis', 'pellentesque', 'tincidunt', 'ante', 'nec', 'justo', 'congue', 'in', 'feugiat', 'metus', 'vehicula', 'nulla', 'facilisi', 'vivamus', 'sagittis', 'lorem', 'a', 'augue', 'viverra', 'at', 'dapibus', 'ex', 'ultrices', 'vestibulum', 'ante', 'ipsum', 'primis', 'in', 'faucibus', 'orci', 'luctus', 'et', 'ultrices', 'posuere', 'cubilia', 'curae', 'pellentesque', 'tincidunt', 'leo', 'quis', 'dui', 'gravida', 'vitae', 'vulputate', 'lorem', 'feugiat']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(texto_sin_signos)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cbb4b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur', 'adipiscing', 'elit', 'sed', 'euismod', 'orci', 'ut', 'commodo', 'placerat', 'sapien', 'sapien', 'convallis', 'nulla', 'nec', 'luctus', 'sapien', 'turpis', 'sit', 'amet', 'massa', 'phasellus', 'facilisis', 'magna', 'et', 'ligula', 'elementum', 'sed', 'volutpat', 'sem', 'malesuada', 'donec', 'sed', 'tincidunt', 'turpis', 'pellentesque', 'tincidunt', 'nec', 'justo', 'congue', 'in', 'feugiat', 'metus', 'vehicula', 'nulla', 'facilisi', 'vivamus', 'sagittis', 'lorem', 'augue', 'viverra', 'at', 'dapibus', 'ex', 'ultrices', 'vestibulum', 'ipsum', 'primis', 'in', 'faucibus', 'orci', 'luctus', 'et', 'ultrices', 'posuere', 'cubilia', 'curae', 'pellentesque', 'tincidunt', 'leo', 'quis', 'dui', 'gravida', 'vitae', 'vulputate', 'lorem', 'feugiat']\n"
     ]
    }
   ],
   "source": [
    "stopwords = (nltk.corpus.stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "tokens_sin_stopwords = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "print(tokens_sin_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74ee0c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur', 'adipiscing', 'elit', 'sed', 'euismod', 'orci', 'ut', 'commod', 'placerat', 'sapi', 'sapi', 'convallis', 'null', 'nec', 'luctus', 'sapi', 'turpis', 'sit', 'amet', 'mass', 'phasellus', 'facilisis', 'magn', 'et', 'ligul', 'elementum', 'sed', 'volutpat', 'sem', 'malesu', 'donec', 'sed', 'tincidunt', 'turpis', 'pellentesqu', 'tincidunt', 'nec', 'just', 'cong', 'in', 'feugiat', 'metus', 'vehicul', 'null', 'facilisi', 'vivamus', 'sagittis', 'lorem', 'aug', 'viverr', 'at', 'dapibus', 'ex', 'ultric', 'vestibulum', 'ipsum', 'primis', 'in', 'faucibus', 'orci', 'luctus', 'et', 'ultric', 'posuer', 'cubili', 'cura', 'pellentesqu', 'tincidunt', 'leo', 'quis', 'dui', 'grav', 'vita', 'vulputat', 'lorem', 'feugiat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Inicializar el stemmer en español\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Aplicar stemming\n",
    "tokens_stemmed = [stemmer.stem(token) for token in tokens_sin_stopwords]\n",
    "\n",
    "print(tokens_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8e8e647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens más frecuentes: [('lorem', 3), ('sed', 3), ('sapi', 3), ('tincidunt', 3), ('ipsum', 2), ('sit', 2), ('amet', 2), ('orci', 2), ('null', 2), ('nec', 2), ('luctus', 2), ('turpis', 2), ('et', 2), ('pellentesqu', 2), ('in', 2), ('feugiat', 2), ('ultric', 2), ('dolor', 1), ('consectetur', 1), ('adipiscing', 1), ('elit', 1), ('euismod', 1), ('ut', 1), ('commod', 1), ('placerat', 1), ('convallis', 1), ('mass', 1), ('phasellus', 1), ('facilisis', 1), ('magn', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "# Calcular la frecuencia de cada token\n",
    "frecuencia_tokens = FreqDist(tokens_stemmed)\n",
    "\n",
    "tokens_mas_frecuentes = frecuencia_tokens.most_common(30)\n",
    "print(\"Tokens más frecuentes:\", tokens_mas_frecuentes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
